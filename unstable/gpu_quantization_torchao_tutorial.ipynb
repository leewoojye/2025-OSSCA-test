{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Google Colab\uc5d0\uc11c \ub178\ud2b8\ubd81\uc744 \uc2e4\ud589\ud558\uc2e4 \ub54c\uc5d0\ub294 \n# https://tutorials.pytorch.kr/beginner/colab \ub97c \ucc38\uace0\ud558\uc138\uc694.\n%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# (prototype) GPU Quantization with TorchAO\n\n**Author**: [HDCharles](https://github.com/HDCharles)\n\nIn this tutorial, we will walk you through the quantization and\noptimization of the popular [segment anything\nmodel](https://github.com/facebookresearch/segment-anything). These\nsteps will mimic some of those taken to develop the\n[segment-anything-fast](https://github.com/meta-pytorch/segment-anything-fast/blob/main/segment_anything_fast/modeling/image_encoder.py#L15)\nrepo. This step-by-step guide demonstrates how you can apply these\ntechniques to speed up your own models, especially those that use\ntransformers. To that end, we will focus on widely applicable\ntechniques, such as optimizing performance with `torch.compile` and\nquantization and measure their impact.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Set up Your Environment\n\nFirst, let\\'s configure your environment. This guide was written for\nCUDA 12.1. We have run this tutorial on an A100-PG509-200 power limited\nto 330.00 W. If you are using a different hardware, you might see\ndifferent performance numbers.\n\n``` {.bash}\n> conda create -n myenv python=3.10\n> pip3 install --pre torch torchvision torchaudio --index-url https://download.pytorch.org/whl/nightly/cu121\n> pip install git+https://github.com/facebookresearch/segment-anything.git\n> pip install git+https://github.com/pytorch/ao.git\n```\n\nSegment Anything Model checkpoint setup:\n\n1.  Go to the [segment-anything repo\n    checkpoint](https://github.com/facebookresearch/segment-anything/tree/main#model-checkpoints)\n    and download the `vit_h` checkpoint. Alternatively, you can use\n    `wget` (for example,\n    `wget https://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth --directory-prefix=<path>`).\n2.  Pass in that directory by editing the code below to say:\n\n``` {.bash}\n{sam_checkpoint_base_path}=<path>\n```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import torch\nfrom torchao.quantization.quant_api import quantize_, Int8DynamicActivationInt8WeightConfig\nfrom torchao.utils import unwrap_tensor_subclass, TORCH_VERSION_AT_LEAST_2_5\nfrom segment_anything import sam_model_registry\nfrom torch.utils.benchmark import Timer\n\nsam_checkpoint_base_path = \"data\"\nmodel_type = 'vit_h'\nmodel_name = 'sam_vit_h_4b8939.pth'\ncheckpoint_path = f\"{sam_checkpoint_base_path}/{model_name}\"\nbatchsize = 16\nonly_one_block = True\n\n\n@torch.no_grad()\ndef benchmark(f, *args, **kwargs):\n    for _ in range(3):\n        f(*args, **kwargs)\n        torch.cuda.synchronize()\n\n    torch.cuda.reset_peak_memory_stats()\n    t0 = Timer(\n        stmt=\"f(*args, **kwargs)\", globals={\"args\": args, \"kwargs\": kwargs, \"f\": f}\n    )\n    res = t0.adaptive_autorange(.03, min_run_time=.2, max_run_time=20)\n    return {'time':res.median * 1e3, 'memory': torch.cuda.max_memory_allocated()/1e9}\n\ndef get_sam_model(only_one_block=False, batchsize=1):\n    sam = sam_model_registry[model_type](checkpoint=checkpoint_path).cuda()\n    model = sam.image_encoder.eval()\n    image = torch.randn(batchsize, 3, 1024, 1024, device='cuda')\n\n    # code to use just a single block of the model\n    if only_one_block:\n        model = model.blocks[0]\n        image = torch.randn(batchsize, 64, 64, 1280, device='cuda')\n    return model, image"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In this tutorial, we focus on quantizing the `image_encoder` because the\ninputs to it are statically sized while the prompt encoder and mask\ndecoder have variable sizes which makes them harder to quantize.\n\nWe'll focus on just a single block at first to make the analysis easier.\n\nLet\\'s start by measuring the baseline runtime.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "try:\n    model, image = get_sam_model(only_one_block, batchsize)\n    fp32_res = benchmark(model, image)\n    print(f\"base fp32 runtime of the model is {fp32_res['time']:0.2f}ms and peak memory {fp32_res['memory']:0.2f}GB\")\n    # base fp32 runtime of the model is 186.16ms and peak memory 6.33GB\nexcept Exception as e:\n    print(\"unable to run fp32 model: \", e)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can achieve an instant performance boost by converting the model to\nbfloat16. The reason we opt for bfloat16 over fp16 is due to its dynamic\nrange, which is comparable to that of fp32. Both bfloat16 and fp32\npossess 8 exponential bits, whereas fp16 only has 4. This larger dynamic\nrange helps protect us from overflow errors and other issues that can\narise when scaling and rescaling tensors due to quantization.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "model, image = get_sam_model(only_one_block, batchsize)\nmodel = model.to(torch.bfloat16)\nimage = image.to(torch.bfloat16)\nbf16_res = benchmark(model, image)\nprint(f\"bf16 runtime of the block is {bf16_res['time']:0.2f}ms and peak memory {bf16_res['memory']: 0.2f}GB\")\n# bf16 runtime of the block is 25.43ms and peak memory  3.17GB"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Just this quick change improves runtime by a factor of \\~7x in the tests\nwe have conducted (186.16ms to 25.43ms).\n\nNext, let\\'s use `torch.compile` with our model to see how much the\nperformance improves.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "model_c = torch.compile(model, mode='max-autotune')\ncomp_res = benchmark(model_c, image)\nprint(f\"bf16 compiled runtime of the block is {comp_res['time']:0.2f}ms and peak memory {comp_res['memory']: 0.2f}GB\")\n# bf16 compiled runtime of the block is 19.95ms and peak memory  2.24GB"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The first time this is run, you should see a sequence of `AUTOTUNE`\noutputs which occurs when inductor compares the performance between\nvarious kernel parameters for a kernel. This only happens once (unless\nyou delete your cache) so if you run the cell again you should just get\nthe benchmark output.\n\n`torch.compile` yields about another 27% improvement. This brings the\nmodel to a reasonable baseline where we now have to work a bit harder\nfor improvements.\n\nNext, let\\'s apply quantization. Quantization for GPUs comes in three\nmain forms in [torchao](https://github.com/pytorch/ao) which is just\nnative pytorch+python code. This includes:\n\n-   int8 dynamic quantization\n-   int8 weight-only quantization\n-   int4 weight-only quantization\n\nDifferent models, or sometimes different layers in a model can require\ndifferent techniques. For models which are heavily compute bound,\ndynamic quantization tends to work the best since it swaps the normal\nexpensive floating point matmul ops with integer versions. Weight-only\nquantization works better in memory bound situations where the benefit\ncomes from loading less weight data, rather than doing less computation.\nThe torchao APIs:\n\n`Int8DynamicActivationInt8WeightConfig()`, `Int8WeightOnlyConfig()` or\n`Int4WeightOnlyConfig()`\n\ncan be used to easily apply the desired quantization technique and then\nonce the model is compiled with `torch.compile` with `max-autotune`,\nquantization is complete and we can see our speedup.\n\n```{=html}\n<div style=\"background-color: #54c7ec; color: #fff; font-weight: 700; padding-left: 10px; padding-top: 5px; padding-bottom: 5px\"><strong>NOTE:</strong></div>\n```\n```{=html}\n<div style=\"background-color: #f3f4f7; padding-left: 10px; padding-top: 10px; padding-bottom: 10px; padding-right: 10px\">\n```\n```{=html}\n<p>You might experience issues with these on older versions of PyTorch. If you runinto an issue, you can use <code>apply_dynamic_quant</code> and<code>apply_weight_only_int8_quant</code> instead as drop in replacement for the twoabove (no replacement for int4).</p>\n```\n```{=html}\n</div>\n```\nThe difference between the two APIs is that the\n`Int8DynamicActivationInt8WeightConfig` API alters the weight tensor of\nthe linear module so instead of doing a normal linear, it does a\nquantized operation. This is helpful when you have non-standard linear\nops that do more than one thing. The `apply` APIs directly swap the\nlinear modules for a quantized module which works on older versions but\ndoesn't work with non-standard linear modules.\n\nIn this case Segment Anything is compute-bound so we'll use dynamic\nquantization:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "del model_c, model, image\nmodel, image = get_sam_model(only_one_block, batchsize)\nmodel = model.to(torch.bfloat16)\nimage = image.to(torch.bfloat16)\nquantize_(model, Int8DynamicActivationInt8WeightConfig())\nif not TORCH_VERSION_AT_LEAST_2_5:\n    # needed for subclass + compile to work on older versions of pytorch\n    unwrap_tensor_subclass(model)\nmodel_c = torch.compile(model, mode='max-autotune')\nquant_res = benchmark(model_c, image)\nprint(f\"bf16 compiled runtime of the quantized block is {quant_res['time']:0.2f}ms and peak memory {quant_res['memory']: 0.2f}GB\")\n# bf16 compiled runtime of the quantized block is 19.04ms and peak memory  3.58GB"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "With quantization, we have improved performance a bit more but memory\nusage increased significantly.\n\nThis is for two reasons:\n\n1)  Quantization adds overhead to the model since we need to quantize\n    and dequantize the input and output. For small batch sizes this\n    overhead can actually make the model go slower.\n2)  Even though we are doing a quantized matmul, such as\u00a0`int8 x int8`,\n    the result of the multiplication gets stored in an int32 tensor\n    which is twice the size of the result from the non-quantized model.\n    If we can avoid creating this int32 tensor, our memory usage will\n    improve a lot.\n\nWe can fix \\#2 by fusing the integer matmul with the subsequent rescale\noperation since the final output will be bf16, if we immediately convert\nthe int32 tensor to bf16 and instead store that we'll get better\nperformance in terms of both runtime and memory.\n\nThe way to do this, is to enable the option `force_fuse_int_mm_with_mul`\nin the inductor config.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "del model_c, model, image\nmodel, image = get_sam_model(only_one_block, batchsize)\nmodel = model.to(torch.bfloat16)\nimage = image.to(torch.bfloat16)\ntorch._inductor.config.force_fuse_int_mm_with_mul = True\nquantize_(model, Int8DynamicActivationInt8WeightConfig())\nif not TORCH_VERSION_AT_LEAST_2_5:\n    # needed for subclass + compile to work on older versions of pytorch\n    unwrap_tensor_subclass(model)\nmodel_c = torch.compile(model, mode='max-autotune')\nquant_res = benchmark(model_c, image)\nprint(f\"bf16 compiled runtime of the fused quantized block is {quant_res['time']:0.2f}ms and peak memory {quant_res['memory']: 0.2f}GB\")\n# bf16 compiled runtime of the fused quantized block is 18.78ms and peak memory  2.37GB"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The fusion improves performance by another small bit (about 6% over the\nbaseline in total) and removes almost all the memory increase, the\nremaining amount (2.37GB quantized vs 2.24GB unquantized) is due to\nquantization overhead which cannot be helped.\n\nWe're still not done though, we can apply a few general purpose\noptimizations to get our final best-case performance.\n\n1)  We can sometimes improve performance by disabling epilogue fusion\n    since the autotuning process can be confused by fusions and choose\n    bad kernel parameters.\n2)  We can apply coordinate descent tuning in all directions to enlarge\n    the search area for kernel parameters.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "del model_c, model, image\nmodel, image = get_sam_model(only_one_block, batchsize)\nmodel = model.to(torch.bfloat16)\nimage = image.to(torch.bfloat16)\ntorch._inductor.config.epilogue_fusion = False\ntorch._inductor.config.coordinate_descent_tuning = True\ntorch._inductor.config.coordinate_descent_check_all_directions = True\ntorch._inductor.config.force_fuse_int_mm_with_mul = True\nquantize_(model, Int8DynamicActivationInt8WeightConfig())\nif not TORCH_VERSION_AT_LEAST_2_5:\n    # needed for subclass + compile to work on older versions of pytorch\n    unwrap_tensor_subclass(model)\nmodel_c = torch.compile(model, mode='max-autotune')\nquant_res = benchmark(model_c, image)\nprint(f\"bf16 compiled runtime of the final quantized block is {quant_res['time']:0.2f}ms and peak memory {quant_res['memory']: 0.2f}GB\")\n# bf16 compiled runtime of the final quantized block is 18.16ms and peak memory  2.39GB"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As you can see, we've squeezed another small improvement from the model,\ntaking our total improvement to over 10x compared to our original. To\nget a final estimate of the impact of quantization lets do an apples to\napples comparison on the full model since the actual improvement will\ndiffer block by block depending on the shapes involved.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "try:\n    del model_c, model, image\n    model, image = get_sam_model(False, batchsize)\n    model = model.to(torch.bfloat16)\n    image = image.to(torch.bfloat16)\n    model_c = torch.compile(model, mode='max-autotune')\n    quant_res = benchmark(model_c, image)\n    print(f\"bf16 compiled runtime of the compiled full model is {quant_res['time']:0.2f}ms and peak memory {quant_res['memory']: 0.2f}GB\")\n    # bf16 compiled runtime of the compiled full model is 729.65ms and peak memory  23.96GB\n\n    del model_c, model, image\n    model, image = get_sam_model(False, batchsize)\n    model = model.to(torch.bfloat16)\n    image = image.to(torch.bfloat16)\n    quantize_(model, Int8DynamicActivationInt8WeightConfig())\n    if not TORCH_VERSION_AT_LEAST_2_5:\n        # needed for subclass + compile to work on older versions of pytorch\n        unwrap_tensor_subclass(model)\n    model_c = torch.compile(model, mode='max-autotune')\n    quant_res = benchmark(model_c, image)\n    print(f\"bf16 compiled runtime of the quantized full model is {quant_res['time']:0.2f}ms and peak memory {quant_res['memory']: 0.2f}GB\")\n    # bf16 compiled runtime of the quantized full model is 677.28ms and peak memory  24.93GB\nexcept Exception as e:\n    print(\"unable to run full model: \", e)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Conclusion\n\nIn this tutorial, we have learned about the quantization and\noptimization techniques on the example of the segment anything model.\n\nIn the end, we achieved a full-model apples to apples quantization\nspeedup of about 7.7% on batch size 16 (677.28ms to 729.65ms). We can\npush this a bit further by increasing the batch size and optimizing\nother parts of the model. For example, this can be done with some form\nof flash attention.\n\nFor more information visit [torchao](https://github.com/pytorch/ao) and\ntry it on your own models.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}